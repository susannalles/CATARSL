<!DOCTYPE html SYSTEM "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <title></title>
    </head>
    <body>
        <div class="talks">
            <div class="author_title"><a
                    href="https://www.holycross.edu/academics/programs/spanish/faculty/francisco-gago-jover"
                    >Francisco Gago Jover</a>, <i>The Old Spanish Textual Archive, or what to do
                    when you find yourself with 30 million words</i></div>
            <div class="bio_abstract">
                <div class="bio"><img alt="" src="http://susannalles.com/CATARSL/img/Gago-Jover.jpg"
                        class="img_bio" /><b>Bio:</b> Francisco Gago-Jover is Professor of Spanish
                    at the College of the Holy Cross. He received his Ph.D. in Hispano Romance
                    Linguistics and Philology at the University of Wisconsin-Madison in 1997 with a
                    dissertation on Medieval Spanish military lexicography. He is the author of two
                    dictionaries, an edition of the Spanish version of the Art of Dying Well,
                    numerous articles on lexicography, and several paleographical transcriptions of
                    medieval Spanish texts. He has taught doctorate courses in different
                    universities in the United States (University of Massachusetts-Amherst and
                    Boston University) and Spain (Universidad de León, Universidad de Valladolid,
                    and Universitat de les Illes Balears). He is the Director of Digital Projects at
                    the Hispanic Seminary of Medieval Studies and is in charge of the Digital
                    Library of the Old Spanish Texts and the Old Spanish Textual Archive.</div>
                <div class="abstract"><b>Abstract:</b>The creation of digital collections of texts,
                    or textual corpora, for research and preservation may be one of the seminal
                    technological innovations in the digital humanities that still remains at the
                    core of many text-oriented disciplines, including those belonging to medieval
                    studies. When creating a textual corpus, digital humanists face many key choices
                    that will determine their project’s success. These decisions include the
                    selection of standards, format types, methods for text recollection,
                    searchability, access, lemmatization, and interoperability, among others. In
                    this talk I will discuss the development of the Old Spanish Textual Archive
                    (OSTA), a morphologically tagged and lemmatized corpus of more than 25 million
                    words, based on the more than 400 semi-paleographic transcriptions of medieval
                    texts written in Castilian, Asturian, Leonese, Navarro-Aragonese and Aragonese
                    prepared by the collaborators of the Hispanic Seminary of Medieval Studies
                    (HSMS).</div>
            </div>
        </div>
        <div class="talks">
            <div class="author_title">James Gawley (University at Buffalo, SUNY), <i>Unsupervised
                    Lemmatization Model for Underserved Languages</i></div>
            <div class="bio_abstract">
                <div class="bio">
                    <img alt="" src="http://susannalles.com/CATARSL/img/Gawley.jpeg" class="img_bio" />
                    <b>Bio:</b> I am a PhD candidate in the Classics Department at UB, and a former
                    Adjunct Instructor of Humanities at Miami-Dade College. I study idea
                    transmission in the ancient world, specifically the allusions found in epic
                    poetry. My approach combines philology with computational linguistics and
                    cognitive science </div>
                <div class="abstract">
                    <b>Abstract:</b> The lemmatization task, wherein a computer model must group
                    together inflected forms derived from the same stem, or 'lemma,' is a
                    fundamental problem in language modeling. Software that handles more complex
                    humanistic tasks like authorship attribution or intertextuality detection relies
                    on lemmatization as an early step. In classical languages, the current standard
                    depends on training sophisticated models with supervised data sets (Burns,
                    2018). These data sets include correct lemmatization tagged by expert readers, a
                    labor intensive task. Modern languages can avoid generating supervised training
                    data by applying machine learning to much larger data sets. Moon and Erk (2008),
                    for example, used an aligned corpus of English and German to infer lemmatization
                    schemes without recourse to hand-tagged training data. Underserved languages do
                    not feature very large aligned corpora, and may not have access to a database of
                    expert annotation for training new models. This paper presents a technique for
                    inferring a lemmatization model for an underserved language without training
                    data beyond a plaintext corpus and a chart of the inflected forms of its
                    lexicon. Tested on the classical Latin, the performance of this unsupervised
                    model is on par with more sophisticated models requiring parsed training data or
                    large aligned corpora. </div>
            </div>
        </div>
        <div class="talks">
            <div class="author_title"><a
                    href="https://www.csun.edu/humanities/english/scott-kleinman">Scott
                Kleinman</a>, <i>Some failed attempts at lemmatising Middle English: what went wrong
                    and where do we go from here?</i></div>
            <div class="bio_abstract">
                <div class="bio">
                    <img alt="Kleinman" src="http://susannalles.com/CATARSL/img/Kleinman.jpg"
                        class="img_bio" />
                    <b>Bio:</b> Scott Kleinman works on medieval language and literature from the
                    Anglo-Saxon period to the fourteenth century with a special emphasis on the
                    regional and cultural diversity of medieval England. in historiographical and
                    romance literature. His publications include studies of legal discourse in
                    Laȝamon’s Brut, the relationship between romance and historiography in Havelok
                    the Dane, the dialectal distribution of the Old English word for “iron”, service
                    cultures in The Lord of the Rings, and topic modelling of Classical Chinese
                    literature. His Digital Humanities work includes the NEH-Funded <a
                        href="https://wheatoncollege.edu/academics/special-projects-initiatives/lexomics/"
                        >Lexomics Project</a>, which studies literature using digital methods and
                    produces the computational text analysis tool <a
                        href="https://wheatoncollege.edu/academics/special-projects-initiatives/lexomics/lexos-installers/"
                        >Lexos</a>. He is also co-Director of the Archive of Early Middle English
                    project, which produces digital editions of English manuscripts written between
                    the eleventh and fourteenth centuries and of the <a
                        href="https://we1s.ucsb.edu/">4Humanities WhatEvery1Says Project</a>, which
                    text mines public discourse in order to produce tools for Humanities advocacy.
                    In the past, he worked as a designer/developer for the online search tool <a
                        href="http://serendip-o-matic.com/">Serendip-o-matic</a> as part of the <a
                        href="http://oneweekonetool.org/">One Week | One Tool project</a>. </div>
                <div class="abstract">
                    <b>Abstract:</b> Middle English may present the trickiest of challenges for
                    automated lemmatisation. With a relatively small corpus of texts, and an even
                    smaller corpus of edited ones, Middle English is not well-suited to
                    machine-learning approaches that rely on scale. Because the corpus is spread
                    amongst dialectal and chronological sub-groups (sometimes single texts) with
                    vastly different grammatical and orthographic features, each group effectively
                    forms its own, even smaller corpus. What is effectively needed is a way for the
                    machine to learn across these sub-corpora. We have a common reference point in
                    the Middle English Dictionary with a pre-defined set of headwords available for
                    use as lemmas. Using the MED has the advantage of potentially providing users
                    with access to the wider semantic and literary contexts of terms in a specific
                    text. In this paper, I will cover some methods I have used to try to match the
                    vocabulary of Middle English texts to MED headwords. These methods include both
                    unsupervised methods of grouping related word types using edit distance metrics,
                    topic models, and word embedding to supervised methods with student research
                    assistants. I will examine some of the problems associated with using the MED,
                    as well as existing corpora containing linguistic markup. Finally, I will
                    examine the promising approach of reinforced machine learning and suggest some
                    ways in which we might pursue this approach through multi-institution
                    collaborations in the future. </div>
            </div>
        </div>
        <div class="talks">
            <div class="author_title"><a href="https://emanjavacas.github.io/">Enrique
                    Manjavacas</a>, <i>Lemmatization of Historical Languages with Neural
                    Networks</i></div>
            <div class="bio_abstract">
                <div class="bio">
                    <img alt="Manjavacas" src="http://susannalles.com/CATARSL/img/Manjavacas.jpg"
                        class="img_bio" />
                    <b>Bio:</b> I am currently a PhD student at the University of Antwerp working on
                    a project in Computational Linguistics. Formally, I am associated with CLiPS and
                    the Antwerp Centre for Digital Humanities and Literary Criticism. I work on the
                    fields of Natural Language Processing and Information Retrieval, focusing on
                    tasks such as morphosyntactic tagging, Stylometry or Text Reuse Detection with
                    applications to historical languages, examining the suitability of Recurrent
                    Neural Network models for such tasks. Besides, I am also interested in the field
                    of Language Generation, where I have focused on text generation with literary
                    and artistic purposes. </div>
                <div class="abstract">
                    <b>Abstract:</b> In the present talk, we will discuss recent approaches to
                    lemmatization based on Neural Network architectures that cast lemmatization as a
                    string transduction task. We will illustrate the approach through a comparison
                    to other data-driven paradigms that make use of contextual information in the
                    form of morphological tags to help solving token-lemma ambiguities. In
                    particular, we will consider a number of Historical Languages and show the
                    effect of the charateristic high degree of orthographic variation on the
                    different approaches to lemmatization. Furthermore, we will discuss and analyze
                    how Neural Network approaches are better suited to counteract such effects.
                    Finally, we will discuss further extensions to the Neural Network approach that
                    lead to further improvements in model fit.</div>
            </div>
        </div>
        <!--<div class="talks">
            <div class="author_title"><a
                href="https://www....">XX</a>, <i>Title</i></div>
            <div class="bio_abstract">
                <div class="bio">
                    <img alt="XXX" src="http://susannalles.com/CATARSL/img/XXX.jpg"
                        class="img_bio" />
                    <b>Bio:</b> ... </div>
                <div class="abstract">
                    <b>Abstract:</b> ... </div>
            </div>
        </div>-->
    </body>
</html>
