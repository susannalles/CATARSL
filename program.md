---
layout: page
title: Program
permalink: /program/
---

(Provisional Program)

## Wednesday, April 24th 2019

Location: [**Newman Alumni Center Executive Boardroom**](https://www.google.com/maps/place/Newman+Alumni+Center,+6200+San+Amaro+Dr,+Coral+Gables,+FL+33146/@25.7114757,-80.2858883,17z/data=!3m1!4b1!4m5!3m4!1s0x88d9c7f942144105:0x517a3c76081d14d!8m2!3d25.7114757!4d-80.2836996)

9-9:30 Coffe and Breakfast

9:30am - 12:30pm - Workshop 1

<div class="talks">
            <div class="author_title"><a href="http://www.mike-kestemont.org/">Mike Kestemont</a> & <a href="https://emanjavacas.github.io/">Enrique Manjavacas</a>, Workshop 1</div>
            <div class="bio_abstract">
                <div class="bio">
                   <img alt="" src="http://susannalles.com/CATARSL/img/logo3.png"/> 
                </div>
                <div class="abstract">
                   <br/>
                </div>
            </div>
</div>
<div></div>
        
Lunch

Location: [Richter Library 3rd Floor Conference Room](https://www.google.com/maps/place/University+of+Miami+Richter+Library/@25.721262,-80.27865,15z/data=!4m2!3m1!1s0x0:0x9a1fae2d87d9c44a?ved=2ahUKEwiv9Zfr0qTgAhVSx1kKHUfSAFMQ_BIwCnoECAYQCA)

2pm - 5pm - Workshop 2

<div class="talks">
            <div class="author_title">Lexos for Analysing Historical Literatures in the Classroom. <a href="">Scott Kleinman</a> (California State University, Northridge) & Mark LeBlanc (Wheaton College, MA)</div>
            <div class="bio_abstract">
                <div class="bio">
                    <img alt="" src="http://susannalles.com/CATARSL/img/Lexos.png"
                        class="img_bio" /></div>
                <div class="abstract">
                  This workshop will demonstrate the capabilities of Lexos for analyzing literatures in historical languages. Lexos is a web-based tool designed for transforming, analyzing, and visualizing texts. Lexos was created by the Lexomics Research Group (Michael Drout at Wheaton College, Scott Kleinman at Cal State Northridge, and Mark LeBlanc at Wheaton College) as an entry-level platform for Humanities scholars and students new to computational techniques while providing tools and techniques sophisticated enough for advanced research. It has been designed specifically to include rich pre-processing features for use in the study of historical languages. Lexos is an open-source web-based tool written in Python, which been under development for ten years with the support of three NEH grants and a unique model in which undergraduates serve as both researchers and software developers. The project has resulted in numerous faculty and student-authored publications (listed <a href="https://wheatoncollege.edu/academics/special-projects-initiatives/lexomics/publications-grants/">here</a>). <a href="http://lexos.wheatoncollege.edu">Lexos</a> may be installed locally; however, we also maintain a live, server-based installation accessible over the internet, making it ideal for classroom use. The tool is accompanied by an embedded Scalar Book (“In the Margins”), which is designed to provide information on algorithms and best practices in computational text analysis. Although “In the Margins” is still under development, we continuously add content (and welcome external contributions) to it, which will ultimately enable users of Lexos to find relevant discussion about the analysis of historical languages embedded within the tool’s workflow.
We propose a 2-3 hour hands-on workshop, in which we will provide an overview of the available features in Lexos, with special attention to functions related to handling premodern texts (e.g. Unicode, XML and HTML entity conversion, TEI markup handling, form consolidation, and lemmatization). We will demonstrate the use of Lexos with some case studies using Old English, Middle English, and/or Latin literature. The case studies will be based on successful experiments used in the classroom or in student research projects to showcase the accessibility of Lexos for teaching early literature as well as computational stylistics. Workshop attendees will be provided with the opportunity to try out their own data sources and to discuss how the tool could be improved to better support the study of historical texts.

                </div>
            </div>
        </div>
        



## Thursday, April 25th 2019

9:30-10am Coffe and Breakfast

10am - 1pm Talks I

<div class="talks">
<div class="author_title"><a href="http://www.mike-kestemont.org/">Mike Kestemont</a> (University of Antwero), <i>The case of fanfiction in Digital Humanities research and pedagogy -- Because the story's not over until they say it is.</i></div>
<div class="bio_abstract">
<div class="bio"><img alt="" src="http://susannalles.com/CATARSL/img/Kestemont.jpg" class="img_bio"/><b>Bio:</b>
 Mike Kestemont, PhD, is a research professor in the department of Literature at the University of Antwerp (Belgium). He specializes in computational text analysis for the Digital Humanities. Whereas his work has a strong focus on historic literature, his precious research has covered a wide range of topics in literary history, including classical, medieval, early modern and modernist texts. Together with Folgert Karsdorp and Allen Riddell he is preparing a textbook on data science for the Humanities. Mike recently took up an interest in neural networks and currently explores various applications of this exciting technology in the Arts and Humanities. Together with his Polish colleagues Maciej Eder and Jan Rybicki he is involved in the Computational Stylistics Group. Mike lives in Brussels (<a href="www.mike-kestemont.org">www.mike-kestemont.org</a>), tweets in English (<a href="https://twitter.com/mike_kestemont?lang=en">@Mike_Kestemont</a>) and codes in Python (<a href="https://github.com/mikekestemont">https://github.com/mikekestemont</a>).</div>
<div class="abstract"><b>Abstract:</b> <i>Forthcoming</i></div>
</div>
</div>

* [Gimena del Río Riande](http://www.iibicrit-conicet.gov.ar/wordpress/quienes-somos/miembros/dra-maria-gimena-del-rio/) (IIBICRIT-CONICET, Argentina), "Computer-assisted Semantic Annotation not just for DH scholars"

<div class="talks">
            <div class="author_title"><a href="http://www.gretafranzini.com/">Greta Franzini</a> & Marco Passarotti (Università Cattolica del Sacro Cuore), <i>LiLa (Linking Latin): Piecing together a seemingly under-resourced language</i>)</div>
            <div class="bio_abstract">
                <div class="bio">
                    <img alt="" src="http://susannalles.com/CATARSL/img/Franzini.jpg"
                        class="img_bio" />
                    <b>Bio:</b> Greta Franzini is a postdoctoral researcher at the Università Cattolica del Sacro Cuore in Milan, Italy. She received her MA and PhD in Digital Humanities at King's College London and University College London, respectively. While her studies concerned the field of <a href="https://dig-ed-cat.acdh.oeaw.ac.at/">Digital Scholarly Editing</a> (notably, the Catalogue of Digital Editions), Greta’s current research interests lie in text analysis and in the application of Natural Language Processing and Computational Linguistics methods to the study of Latin. Prior to joining Università Cattolica, she contributed to the development and evaluation of <a href="https://www.etrap.eu/research/tracer/">TRACER</a>, a text reuse detection machine, to detect literal quotations and paraphrase in Latin texts.
Alongside her research, Greta serves as journal editor for <a href="https://umanisticadigitale.unibo.it/">Umanistica Digitale</a> and <a href="https://journal.digitalmedievalist.org/">Digital Medievalist</a>, and as a board member of the <a href="http://www.aiucd.it/">Italian Association of Digital Humanities (AIUCD)</a>.
                </div>
                 <div class="bio">
                    <b>Bio:</b> Marco Passarotti is Associate Professor at the Università Cattolica del Sacro Cuore in Milan, Italy. His main research interests deal with building, using and disseminating linguistic resources and Natural Language Processing tools for Latin. A pupil of father Roberto Busa SJ, one of the pioneers of Humanities Computing, since 2006 Marco directs the <a href="https://itreebank.marginalia.it/">Index Thomisticus Treebank</a> project. In 2009, he founded the CIRCSE Research Centre of Computational Linguistics at Università Cattolica.
Currently, he is Principal Investigator of an ERC Grant (2018-2023) aimed at building a Linked Data Knowledge Base of resources and tools for Latin (LiLa). He has organized and chaired several international scientific events. He co-chairs the series of workshops on 'Corpus-based Research in the Humanities' (CRH). He teaches Computational Linguistics at Università Cattolica (Milan) and at the University of Pavia. 
                </div>
                <div class="abstract">
                    <b>Abstract:</b> The LiLa: Linking Latin project was recently awarded funding from the European Research Council (ERC) to build a Knowledge Base of linguistic resources for Latin. LiLa responds to the growing need in the fields of Computational Linguistics, Humanities Computing and Classics to create an interoperable ecosystem of resources and Natural Language Processing (NLP) tools for Latin. To this end, LiLa makes use of Linked Open Data (LOD) practices and standards to connect words to distributed textual and lexical resources via unique identifiers. In so doing, LiLa builds rich knowledge graphs, which can be used for research and teaching purposes alike.  
The talk will detail the current state of LiLa and shed light on common misconceptions about Latin being an under-resourced language.
                </div>
            </div>
        </div>

Lunch

2pm - 5pm Talks II

<div class="talks">
<div class="author_title"><a href="https://www.holycross.edu/academics/programs/spanish/faculty/francisco-gago-jover">Francisco Gago Jover</a>, <i>The Old Spanish Textual Archive, or what to do when you find yourself with 30 million words</i></div>
<div class="bio_abstract">
<div class="bio"><img alt="" src="http://susannalles.com/CATARSL/img/Gago-Jover.jpg" class="img_bio"/><b>Bio:</b>
 Francisco Gago-Jover is Professor of Spanish at the College of the Holy Cross. He received his Ph.D. in Hispano Romance Linguistics and Philology at the University of Wisconsin-Madison in 1997 with a dissertation on Medieval Spanish military lexicography. He is the author of two dictionaries, an edition of the Spanish version of the Art of Dying Well, numerous articles on lexicography, and several paleographical transcriptions of medieval Spanish texts. He has taught doctorate courses in different universities in the United States (University of Massachusetts-Amherst and Boston University) and Spain (Universidad de León, Universidad de Valladolid, and Universitat de les Illes Balears). He is the Director of Digital Projects at the Hispanic Seminary of Medieval Studies and is in charge of the Digital Library of the Old Spanish Texts and the Old Spanish Textual Archive.</div>
<div class="abstract"><b>Abstract:</b>The creation of digital collections of texts, or textual corpora, for research and preservation may be one of the seminal technological innovations in the digital humanities that still remains at the core of many text-oriented disciplines, including those belonging to medieval studies. When creating a textual corpus, digital humanists face many key choices that will determine their project’s success. These decisions include the selection of standards, format types, methods for text recollection, searchability, access, lemmatization, and interoperability, among others. In this talk I will discuss the development of the Old Spanish Textual Archive (OSTA), a morphologically tagged and lemmatized corpus of more than 25 million words, based on the more than 400 semi-paleographic transcriptions of medieval texts written in Castilian, Asturian, Leonese, Navarro-Aragonese and Aragonese prepared by the collaborators of the Hispanic Seminary of Medieval Studies (HSMS).</div>
</div>
</div>

<div class="talks">
<div class="author_title">James Gawley (University at Buffalo, SUNY), <i>Unsupervised Lemmatization Model for Underserved Languages</i></div>
<div class="bio_abstract">
  <div class="bio">
                    <img alt="" src="http://susannalles.com/CATARSL/img/Gawley.jpeg"
                        class="img_bio" />
                    <b>Bio:</b> I am a PhD candidate in the Classics Department at UB, and a former Adjunct Instructor of Humanities at Miami-Dade College. I study idea transmission in the ancient world, specifically the allusions found in epic poetry. My approach combines philology with computational linguistics and cognitive science
                </div>
                <div class="abstract">
                    <b>Abstract:</b> The lemmatization task, wherein a computer model must group together inflected forms derived from the same stem, or 'lemma,' is a fundamental problem in language modeling. Software that handles more complex humanistic tasks like authorship attribution or intertextuality detection relies on lemmatization as an early step.

In classical languages, the current standard depends on training sophisticated models with supervised data sets (Burns, 2018). These data sets include correct lemmatization tagged by expert readers, a labor intensive task. Modern languages can avoid generating supervised training data by applying machine learning to much larger data sets. Moon and Erk (2008), for example, used an aligned corpus of English and German to infer lemmatization schemes without recourse to hand-tagged training data. Underserved languages do not feature very large aligned corpora, and may not have access to a database of expert annotation for training new models.

This paper presents a technique for inferring a lemmatization model for an underserved language without training data beyond a plaintext corpus and a chart of the inflected forms of its lexicon. Tested on the classical Latin, the performance of this unsupervised model is on par with more sophisticated models requiring parsed training data or large aligned corpora.
                </div>
            </div>
        </div>

<div class="talks">
            <div class="author_title"><a href="https://www.csun.edu/humanities/english/scott-kleinman">Scott Kleinman</a>, <i>Some failed attempts at lemmatising Middle English: what went wrong and where do we go from here?</i></div>
            <div class="bio_abstract">
                <div class="bio">
                    <img alt="Kleinman" src="http://susannalles.com/CATARSL/img/Kleinman.jpg"
                        class="img_bio" />
                    <b>Bio:</b> Scott Kleinman works on medieval language and literature from the Anglo-Saxon period to the fourteenth century with a special emphasis on the regional and cultural diversity of medieval England. in historiographical and romance literature. His publications include studies of legal discourse in Laȝamon’s Brut, the relationship between romance and historiography in Havelok the Dane, the dialectal distribution of the Old English word for “iron”, service cultures in The Lord of the Rings, and topic modelling of Classical Chinese literature. His Digital Humanities work includes the NEH-Funded <a href="https://wheatoncollege.edu/academics/special-projects-initiatives/lexomics/">Lexomics Project</a>, which studies literature using digital methods and produces the computational text analysis tool <a href="https://wheatoncollege.edu/academics/special-projects-initiatives/lexomics/lexos-installers/">Lexos</a>. He is also co-Director of the Archive of Early Middle English project, which produces digital editions of English manuscripts written between the eleventh and fourteenth centuries and of the <a href="https://we1s.ucsb.edu/">4Humanities WhatEvery1Says Project</a>, which text mines public discourse in order to produce tools for Humanities advocacy. In the past, he worked as a designer/developer for the online search tool <a href="http://serendip-o-matic.com/">Serendip-o-matic</a> as part of the <a href="http://oneweekonetool.org/">One Week | One Tool project</a>.
                </div>
                <div class="abstract">
                    <b>Abstract:</b> Middle English may present the trickiest of challenges for automated lemmatisation. With a relatively small corpus of texts, and an even smaller corpus of edited ones, Middle English is not well-suited to machine-learning approaches that rely on scale. Because the corpus is spread amongst dialectal and chronological sub-groups (sometimes single texts) with vastly different grammatical and orthographic features, each group effectively forms its own, even smaller corpus. What is effectively needed is a way for the machine to learn across these sub-corpora. We have a common reference point in the Middle English Dictionary with a pre-defined set of headwords available for use as lemmas. Using the MED has the advantage of potentially providing users with access to the wider semantic and literary contexts of terms in a specific text. In this paper, I will cover some methods I have used to try to match the vocabulary of Middle English texts to MED headwords. These methods include both unsupervised methods of grouping related word types using edit distance metrics, topic models, and word embedding to supervised methods with student research assistants. I will examine some of the problems associated with using the MED, as well as existing corpora containing linguistic markup. Finally, I will examine the promising approach of reinforced machine learning and suggest some ways in which we might pursue this approach through multi-institution collaborations in the future.
                </div>
            </div>
        </div>

5 - 5:30pm Closing Discussion
